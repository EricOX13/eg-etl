{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=ISO-8859-1\n",
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=ISO-8859-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/ericyeung/Library/Caches/pypoetry/virtualenvs/evergreen-DHscqOYO-py3.11/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/ericyeung/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/ericyeung/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.4_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-22b29bc2-d5f9-436e-a870-b71c5d3d6675;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.4_2.12;1.4.3 in central\n",
      ":: resolution report :: resolve 82ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.4_2.12;1.4.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-22b29bc2-d5f9-436e-a870-b71c5d3d6675\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/4ms)\n",
      "24/02/29 22:02:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240229_220216\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from datetime import datetime\n",
    "\n",
    "warehouse_path = \"./warehouse\"\n",
    "iceberg_spark_jar  = 'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3'\n",
    "catalog_name = \"demo\"\n",
    "\n",
    "# Setup iceberg config\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"EgApp\")\n",
    "    .set(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.jars.packages\", iceberg_spark_jar)\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}.warehouse\", warehouse_path)\n",
    "    .set(f\"spark.sql.catalog.{catalog_name}.type\", \"hadoop\")\n",
    "    .set(\"spark.sql.defaultCatalog\", catalog_name)\n",
    ")\n",
    "\n",
    "sc = SparkSession.builder.master(\"local\").config(conf=conf).getOrCreate()\n",
    "\n",
    "batch = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from conf.init_schema import init_database\n",
    "\n",
    "init_database(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postcode related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_utils import extract_csv\n",
    "\n",
    "df_postcode_lookup = extract_csv(sc, \"src_data/National_Statistics_Postcode_Lookup_Latest_Centroids.csv\")\n",
    "df_postcode_lookup.printSchema()\n",
    "df_postcode_lookup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.postcode import PostcodeTransformer\n",
    "df_postcode = PostcodeTransformer.transform(df_postcode_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode.write.mode(\"overwrite\").insertInto(f\"{catalog_name}.db.postcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "users_schema = StructType(\n",
    "[StructField(\"email\", StringType(), True),\n",
    " StructField(\"givenName\", StringType(), True),\n",
    " StructField(\"familyName\", StringType(), True),\n",
    " StructField(\"sex\", StringType(), True),\n",
    " StructField(\"dateOfBirth\", StringType(), True),\n",
    " StructField(\"address\", StructType([\n",
    "     StructField(\"street\", StringType(), True),\n",
    "     StructField(\"postcode\", StringType(), True)]), True),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_utils import extract_json\n",
    "\n",
    "df_users = extract_json(sc, \"src_data/users.json\", users_schema)\n",
    "df_users.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_utils import check_data_quality\n",
    "\n",
    "df_validated = check_data_quality(df_users, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.users import UserTransformer\n",
    "\n",
    "df_transformed = UserTransformer.transform(df_validated)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_utils import write_to_iceberg\n",
    "\n",
    "write_to_iceberg(sc, df_transformed, \"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.sql(\"select count(1) from db.users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.createOrReplaceTempView(\"temp_source\")\n",
    "\n",
    "query = f\"\"\"\n",
    "    MERGE INTO db.users as t USING temp_source as s\n",
    "            ON (s.last_name = t.last_name)\n",
    "            WHEN MATCHED AND s.first_name = t.first_name AND s.last_name = t.last_name AND s.dob = t.dob THEN UPDATE SET t.first_name = s.first_name \n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\"\n",
    "\n",
    "insert_query = \"insert into db.users select * from temp_source\"\n",
    "\n",
    "sc.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.sql(\"select count(1) from db.users t join temp_source s where s.last_name = t.last_name\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
